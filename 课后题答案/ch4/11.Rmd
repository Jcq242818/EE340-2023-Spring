11
========================================================

### a
```{r}
library(ISLR)
summary(Auto)
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg>median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
```

### b
```{r 11b}
cor(Auto[,-9])
pairs(Auto) # doesn't work well since mpg01 is 0 or 1
```
Anti-correlated with cylinders, weight, displacement, horsepower.
(mpg, of course)

### c
```{r}
train = (year %% 2 == 0) # if the year is even
test = !train
Auto.train = Auto[train,]
Auto.test = Auto[test,]
mpg01.test = mpg01[test]
```

### d
```{r}
# LDA
library(MASS)
lda.fit = lda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
lda.pred = predict(lda.fit, Auto.test)
mean(lda.pred$class != mpg01.test)
```
12.6% test error rate.

### e
```{r}
# QDA
qda.fit = qda(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto, subset=train)
qda.pred = predict(qda.fit, Auto.test)
mean(qda.pred$class != mpg01.test)
```
13.2% test error rate.

### f
```{r}
# Logistic regression
glm.fit = glm(mpg01~cylinders+weight+displacement+horsepower,
              data=Auto,
              family=binomial,
              subset=train)
glm.probs = predict(glm.fit, Auto.test, type="response")
glm.pred = rep(0, length(glm.probs))
glm.pred[glm.probs > 0.5] = 1
mean(glm.pred != mpg01.test)
```
12.1% test error rate.

### g
```{r}
library(class)
train.X = cbind(cylinders, weight, displacement, horsepower)[train,]
test.X = cbind(cylinders, weight, displacement, horsepower)[test,]
train.mpg01 = mpg01[train]
set.seed(1)
# KNN(k=1)
knn.pred = knn(train.X, test.X, train.mpg01, k=1)
mean(knn.pred != mpg01.test)
# KNN(k=10)
knn.pred = knn(train.X, test.X, train.mpg01, k=10)
mean(knn.pred != mpg01.test)
# KNN(k=100)
knn.pred = knn(train.X, test.X, train.mpg01, k=100)
mean(knn.pred != mpg01.test)
```
k=1, 15.4% test error rate.
k=10, 16.5% test error rate.
k=100, 14.3% test error rate.
K of 100 seems to perform the best. 100 nearest neighbors.
