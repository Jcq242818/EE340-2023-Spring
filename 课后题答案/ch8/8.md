Chapter 8: Exercise 8
========================================================

### a

```r
library(ISLR)
attach(Carseats)
set.seed(1)

train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
```


### b

```r
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
```

```
## 
## Regression tree:
## tree(formula = Sales ~ ., data = Carseats.train)
## Variables actually used in tree construction:
## [1] "ShelveLoc"   "Price"       "Age"         "Advertising" "Income"     
## [6] "CompPrice"  
## Number of terminal nodes:  18 
## Residual mean deviance:  2.36 = 429 / 182 
## Distribution of residuals:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -4.260  -1.040   0.102   0.000   0.930   3.910
```

```r
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

![plot of chunk b8](figure/b8.png) 

```r
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
```

```
## [1] 4.149
```

The test MSE is about $4.15$.

### c

```r
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b")
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

![plot of chunk 8c](figure/8c1.png) 

```r

# Best size = 9
pruned.carseats = prune.tree(tree.carseats, best = 9)
par(mfrow = c(1, 1))
plot(pruned.carseats)
text(pruned.carseats, pretty = 0)
```

![plot of chunk 8c](figure/8c2.png) 

```r
pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
```

```
## [1] 4.993
```

Pruning the tree in this case increases the test MSE to $4.99$.

### d

```r
library(randomForest)
```

```
## randomForest 4.6-7
## Type rfNews() to see new features/changes/bug fixes.
```

```r
bag.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, 
    importance = T)
bag.pred = predict(bag.carseats, Carseats.test)
mean((Carseats.test$Sales - bag.pred)^2)
```

```
## [1] 2.586
```

```r
importance(bag.carseats)
```

```
##             %IncMSE IncNodePurity
## CompPrice   13.8790       131.095
## Income       5.6042        77.033
## Advertising 14.1720       129.218
## Population   0.6071        65.196
## Price       53.6119       506.604
## ShelveLoc   44.0311       323.189
## Age         20.1751       189.269
## Education    1.4244        41.811
## Urban       -1.9640         8.124
## US           5.6989        14.307
```

Bagging improves the test MSE to $2.58$. We also see that $\tt{Price}$, $\tt{ShelveLoc}$ and $\tt{Age}$ are three most important predictors of $\tt{Sale}$.

### e

```r
rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 5, ntree = 500, 
    importance = T)
rf.pred = predict(rf.carseats, Carseats.test)
mean((Carseats.test$Sales - rf.pred)^2)
```

```
## [1] 2.87
```

```r
importance(rf.carseats)
```

```
##             %IncMSE IncNodePurity
## CompPrice   11.2746        126.64
## Income       4.4397        101.63
## Advertising 12.9346        137.96
## Population   0.2725         78.78
## Price       49.2418        449.52
## ShelveLoc   38.8406        283.46
## Age         19.1329        195.14
## Education    1.9818         54.26
## Urban       -2.2083         11.35
## US           6.6487         26.71
```

In this case, random forest worsens the MSE on test set to $2.87$. Changing $m$ varies test MSE between $2.6$ to $3$. We again see that $\tt{Price}$, $\tt{ShelveLoc}$ and $\tt{Age}$ are three most important predictors of $\tt{Sale}$.
